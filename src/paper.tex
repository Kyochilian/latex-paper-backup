\documentclass[conference]{IEEEtran}   % 单栏会议模板；去 conference 选项即双栏
\usepackage[UTF8, scheme = plain]{ctex} % 中文支持
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% ---------------- 常用 AI 宏包 ----------------
\usepackage{amsmath, amssymb, amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}   % 伪代码
\usepackage{graphicx}
\usepackage{booktabs}        % 三线表
\usepackage{multirow}
\usepackage{array}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{cite}            % IEEE 引用风格
\usepackage{xcolor}
\usepackage{microtype}

% ---------------- 定理环境 ----------------
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}

% ---------------- 自定义命令 ----------------
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\etal}{\textit{et al.}}

% ---------------- 正文开始 ----------------
\begin{document}

\title{基于Stage扫描机制的Vision RWKV让医学影像分割轻量而高效}

\author{%
  \IEEEauthorblockN{sora\IEEEauthorrefmark{1},
                    Zhendong Li\IEEEauthorrefmark{1},
                    }
  \IEEEauthorblockA{\IEEEauthorrefmark{1}Ningxia University\\
                    Email: kyochilian@gmail.com}
  \IEEEauthorblockA{\IEEEauthorrefmark{1}Ningxia University\\
                    Email: lizhendong@nxu.edu.cn}
}

\maketitle

\begin{abstract}
基于Transformer的分割框架是当前医学影像分割的主流方法，能够构建全局关系。
然而，在需要高精度与高分辨率的医学图像分割中，Transformer计算复杂度高，应用有限。
最近的RWKV模型降低了空间聚合复杂度并具备全局处理能力，但其在视觉领域存在长程建模羸弱的问题，贪吃蛇效应突出，同时循坏嵌套机制在分割领域中的解码性能不足。
为了解决这些问题，我们提出了一种基于Vision RWKV的分割框架RWKV-SSS，该框架在具有较低复杂度的同时具备了全局建模能力。
为了增强RWKV的长程空间连续性，我们设计了一种Stage扫描机制，该机制将图像分为全局block与局部block，根据不同Stage的编码器动态地调整扫描权重，
将全局与局部特征融合，大大提升了模型的精度。
同时，我们在解码器中构建了上采样模块---，该模块改良了RWKV的循环嵌套机制，提升了重建多尺度特征的能力。
我们还采用多种方法进一步降低SSS的计算复杂度，实现模型的轻量化。（LRFormer 低分辨率自注意力，池化QKV等等）
实验表明，我们的方法在多个数据集取得优异性能，同时计算复杂度与计算速度显著增强，使其在高精度分割的同时具备轻量化特性。
代码开源至：\url{https://github.com/shepherdxu/SCI-winning}。
\end{abstract}

\begin{IEEEkeywords}
computer vision, semantic segmentation, RWKV, Transformer, lightweight
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{S}{emantic} Segmentation 是计算机视觉中的一项重要任务，医学影像分割是关键应用之一。它将复杂的医学图像分割成不同的区域，
使器官、病灶及注意区域清晰可见，对于医学辅助诊断与治疗具有重要意义。
相较于人工，使用计算机视觉技术进行辅助诊断不仅提高了诊断效率，还提升了诊断精度，因此许多方法被提出用于医学影像分割任务。
多年来，传统的ML（机器学习）方法在研究中被广泛应用，如基于图割（Graph Cut）的方法~\cite{Graph Cut}、随机森林（Random Forest）~\cite{Random Forest}、
支持向量机(Support Vector Machine, SVM)~\cite{SVM}以及条件随机场(Conditional Random Field, CRF)~\cite{Conditional Random Field}等。
这些方法依赖人工设计的特征提取器,在处理复杂医学图像时存在局限性，
由于图像存在模糊、噪声、对比度低等问题，传统方法的准确性和鲁棒性受到限制。

近年来，随着FCN(Full Convolution Network)~\cite{FCN}、CNN(Convolution Natural Network)~\cite{CNN}的提出，深度学习方法逐渐代替了传统机器学习方法，
语义分割方法的网络深度、特征提取均有了较大提高。
随着Unet~\cite{Unet}的提出，基于深度学习的分割方法占据了主导地位，通过创新性的U型架构，使其成为了语义分割的主流框架，涌现了Unet++~\cite{unet++}、
Attention-Unet~\cite{attetion-unet}等方法。
2017年，Google提出的Transformer~\cite{Transformer}在自然语言处理（NLP）领域have maked深远影响，which为视觉领域带来了ViT(Vision Transformer)~\cite{ViT} in 2020，
使得计算机视觉领域拥有了全局注意力(Global Attention)与自注意力机制(Self-Attention)~\cite{Transformer}，makes better for全局语义信息提取，改善了CNN的不足~\cite{lack for CNN}。
**等人提出了TransUnet~\cite{TransUnet}，将Unet与CNN和Transformer结合起来，使得医学图像分割方法的精度大大提高。

目前，主流的方法主要遵循TransUnet的思路，在上述深度学习方法中作修改~\cite{分割方法综述}，将CNN作为局部注意力提取~\cite{EViT-Unet}，为了进行更高精度的分辨，
将Transformer作为把握全局特征的模块，采用类似Unet的上下采样与skip connection的结构，达到期望的分割
效果~\cite{Medical Image Segmentation: A Comprehensive Review of Deep Learning Methods}。
然而，医学图像具有低对比度、高分辨率、目标边界模糊等诸多问题~\cite{feature of medical img}，对于自注意力机制来说缺乏友好，往往需要花费大量的参数来提取低对比度医学特征，
牺牲了时间与空间性能~\cite{lack of Transformer on medical image}，
同时自注意力机制具有平方复杂度，高分辨率的医学图像会使计算复杂度大大增加，高参数网络也不适宜部署在医院等边缘设备，这些限制了Transformer在医学图像上的直接应用~\cite{transunet}
~\cite{h2former}。

一些研究人员研究改善这种情况。****等人提出了Linear Transformer~\cite{Linear Transformer}，**等人在20xx年提出了Mamba~\cite{Mamba}模型，不同于Transformer的框架，
他们均通过将自注意力机制的复杂度改为线性，从而缓解计算复杂度问题。然而，事实证明，线性复杂度的Transformer变体总会牺牲分割精度与准确率，而Mamba模型的长序列建模（SSM）
也不适用于视觉问题~\cite{mamba out}。鉴于医学影像中 3D 体数据、MRI与CT等高分辨率数据普遍存在，且存在高精度的像素级分割要求，
实践中还需要考虑医院边缘设备的部署问题，如何平衡精度与效率、参数与性能，成为急需解决的问题。

Recurrent Weighted Key-Value(RWKV)~\cite{rwkv}的出现引起了我们的关注。其在NLP领域的线性复杂度注意力机制，不同于Transformer的结构，使其有价值迁移到视觉任务上。
Vision RWKV(VRWKV)~\cite{visionrwkv}尝试了该问题，并针对图像输入进行了结构改进，展现出优异的计算效率与模型精度，**等人提出了BSBP-RWKV~\cite{BSBP-RWKV}，
首次将RWKV应用于医学图像任务，一些研究如RWKV-Unet~\cite{RWKV-UNet}将VRWKV与Unet迁移到医学图像分割任务上。在高分辨率的医学图像任务下，
RWKV优于其他Transformer变体与Mamba模型，处理医学图像输入时的推理速度更高，且效果更好~\cite{rwkv-sam}。

作为一款线性注意力模型，尽管RWKV拥有良好的计算效率与精度，但迁移到图像任务上，还是有不可避免的问题。RWKV本质是沿序列建模的状态空间式结构,
针对离散数据(如自然语言处理)的处理过程是固定的，自注意力的处理方式也是一维的。面对连续且二维的图像，基于RWKV的图像处理方法是将图像分块，
并延展为一维，导致了空间连续性上的破坏~\cite{zigrir}。

在注意力提取过程中，RWKV使用一维的方式在二维图像上进行特征提取，我们称其为贪吃蛇效应~\ref{fig:贪吃蛇效应}。
RWKV的扫描方式是固定的，在图像任务中，注意力的权重往往动态且多变，固定的扫描方式往往会将注意力分片~\cite{visionrwkv}。(如图~\ref{fig:图RWKV的空间连续性破坏})。
若提取到更深层次，图像的特征更容易模糊，RWKV容易将模糊的特征边缘分离，进行分片扫描，导致容易导致器官边缘、细小病灶（如微小息肉、早期病变区）分割不精细，
而这是医学图像分割的要求之一(如图~\ref{fig:RWKV分片将深层次的边缘分割开来})。
同时，医学方向涉及三维数据，在三维医学图像中，边界分割、注意力分片的现象将会更为突出，这为RWKV向医学领域的适配带来了挑战~\cite{med-urwkv}。

为此，很多工作进行了改进。U-RWKV提出了方向自适应 RWKV 模块~\cite{urwkv}，改进了RWKV的扫描方式，使其不仅仅局限于一维层面的注意力叠加。Zig-RiR~\cite{zigrir}提出了
ZigZag的扫描方案，试图解决RWKV扫描的空间连续性问题。然而，其带来了诸多问题：扫描缺乏对图像的动态调整，导致其训练过程不稳定，且不同器官之间的精度存在较大差异
~\ref{fig:ZigRiR与URWKV的训练loss曲线不稳定};同时，贪吃蛇效应仍然存在；两者工作的上采样过于简单，导致恢复特征分辨率时的精度不足，无法满足医学图像像素级分割的要求。

我们的研究致力于解决该问题。受到Vision RWKV~\cite{vrwkv}以及URWKV~\cite{urwkv}的启发，我们提出了一种基于RWKV的U型架构Stage Scan Segmentation，
该架构具备完备的编码器-解码器，在保持了RWKV的长程依赖和线性复杂度的同时，增强了局部细节的表达与分割的精度。具体来说，我们采用了Bi-WKV，一种线性注意力，
作为我们的注意力核心~\cite{vrwkv}，并提出了stage scan，一种基于encoder层次的扫描机制，根据不同层次的编码器动态地调整扫描方向与权重。
为了解决RWKV的空间连续性问题，SSS将一个维度的模块设置为全局block与局部block，分别提取全局与局部特征。在一开始，输入特征的分辨率
与H、W较大，特征比较分散，噪声较多，需要我们进行更多的分块扫描。随着编码器的Stage增加，输入特征的细节将会更多更集中，
图像的分块数也会随着层数的增加而减少~\ref{fig:不同层次的图像分块}，以此把握全局的注意力。在一个模块的前端，
我们使用全局block，较大的Stage分块，进行初步的特征提取。随后，采用残差机制~\cite{resnet}将提取后的全局权重与原特征加和。
局部block接收加和后的权重，将stage分块进一步增大，进行局部
的特征提取。同时，为了缓解贪吃蛇效应~\ref{fig:贪吃蛇效应}，我们改进了四项token位移~\cite{vrwkv}，根据已确定的扫描机制，动态调整扫描位移的方向。
低参数量~\ref{fig:计算效率与参数量对比}。其次，我们对RWKV的WKV进行适当池化~\cite{lrformer}，实现进一步的轻量化。
最后，我们在上采样阶段实现动态上采样~\cite{dysample}，而不仅仅进行反卷积，进一步保留了分割结果的精度与细节。
通过这种方式，SSS获得优于其他方法的特征捕捉能力~\ref{fig:效果对比图}，增强了对边缘的分割能力和鲁棒性，并发挥了RWKV应有的计算效率与低参数量。

总的来说，我们的贡献有：

1. 使用VRWKV作为分割的核心方法，达到了线性复杂度与注意力精度的平衡；

2. 提出了基于Stage的Scan方法，缓解了贪吃蛇效应，and改进了上采样，有效提取全局特征与局部特征；

3. 在对多个2d以及3d数据集中，SSS的效果优于目前许多最先进的方法，同时参数量降低了20\%,GPU占用降低了23\%~\ref{fig:资源占用与GPU占用对比图}。


待写：
上采样问题
轻量化问题 所谓的动态？




加入RWKV扫描后的热力图，表示注意力在图像上的衰减



\section{Related Works}
\subsection{医学图像分割}
目前，基于深度学习的医学图像分割网络是主流，且很多的先进方法均基于CNN和Transformer的变体得来。

\textit{1. 纯净的CNN。}最具代表性的为Unet~\cite{Unet}。UNet的优势在于编码器-解码器的结构，并通过跳
跃连接有效减少信息在传输过程中的损失，实现对复杂医学图像中细微结构的精准分割。部分研究者对Unet进行了改进，例如
UNet++、UNet3+、3D UNET和Attention Unet等等。但是，卷积核的有限感受野难以捕捉远程关系和远程关系，限制了模型的分割性能。
同时，在面对大尺寸医学图像或三维图像时，UNet的性能往往不足，导致细节的丢失和全局能力的不足。

\textit{2. 纯净的Transformer。}Transformer的核心机制self-attention使其能够把握全局上下文关系，该机制被大量应用至计算机视觉领域，例如ViT。
Transformer克服了CNN在处理长程依赖关系时的不足，并将每个图像块作为输入序列传递到 Transformer 编码器，以获得图像的全局表示。Liu~\cite{swintransformer}等人设计了
SwinTransformer架构，该架构针对图像分割和目标检测任务设计，并在医学影像分割上取得了不错的成绩。Cao~\cite{swinunet}等人将Unet与Transformer架构融合，
设计了Swin-Unet，提升了医学影像的分割精度和泛化能力。Lin~\cite{dstransunet}等人进行了进一步的改进，采用双尺度编码器子网来提取不同语义尺度的特征，
建立不同尺度之间的依赖关系。采用纯净Transformer的模型对医学图像的精度和鲁棒性较好，但其复杂度较高，精细局部特征提取能力不足，限制了其在医学图像中的应用。


\textit{3. 将CNN与Transformer结合，}利用CNN在捕捉局部特征上的优越性以及Transformer对全局依赖性的处理能力。具有代表性的是TransUNet~\cite{transunet}、
CoTr~\cite{cotr}、UTNet~\cite{utnet}和Transfuse~\cite{transfuse}网络等。这些网络通过各种串行、并行的方式将CNN与Transformer结合到一起，
整合了全局与局部特征，使精度进一步提高。该方法的核心问题是：模型计算复杂度较高。Transformer 模块处理图像任务需要将图像序列化，形成较长的序列。
尽管 ViT 提出了图像块序列，但图像切割后计算量仍然较大；混合模型需要同时维护UNet的卷积层和高复杂度的自注意力机制，虽然增强了细节的
特征提取能力，但模型的参数和推理速度均显著降低，无法部署在医院的大量边缘及低算力设备中，应用有限。


改进~\cite{Medical Image Segmentation: A Comprehensive Review of Deep Learning Methods}。


\subsection{线性注意力}
线性注意力致力于打破二次方级的注意力复杂度瓶颈，使注意力机制更加高效，并催生出了一系列Transformer变体和非Transformer架构。

\textit{1. Transformer变体。}一些针对于Transformer的改进如Linear Transformer~\cite{lineartransformer}，提出了特征映射的概念，用以替代Softmax，
将Attention机制视为Linear RNN的过程。**等人提出了Performer~\cite{performer}，引入了正交随机特征（Orthogonal Random Features, ORF），
使其接近Softmax的效果。二者均通过对softmax函数效果的改进，进而达到线性注意力的目的。以此为基础，Han~\cite{flatten transformer}等人提出了Flatten Transformer，
设计了一种聚焦线性注意力（Focused Linear Attention），改善了空间特征的完整性，完善了线性注意力在视觉领域的应用。

\textit{2. the new wave。}如经典的mamba~\cite{mamba}模型，其引入了选择性扫描机制（Selective Scan），让模型能够根据输入内容动态压缩记忆。
U-Mamba~\cite{umamba}、Swin-UMamba~\cite{swinumamba}、VM-UNet~\cite{vmunet}等方法探索了Mamba模型在医学图像分割领域的运用，将Mamba模型与SSM的思想
整合进了分割领域当中，并进行了针对性的优化。然而，SSM的核心机制致使其无法有效建模初窗口之外的全局上下文信息，基于SSM的视觉模型与最先进的卷积模型和基于注意力的模型相比，
表现不佳，导致其并不适合视觉任务~\cite{mamba out}，且其运行速度往往以准确率为代价。

\subsection{RWKV}
最近提出的Receptance Weighted Key Value(RWKV) ~\cite{RWKV}是不同于Transformer的线性架构，擅长处理长序列关系与自回归任务，在NLP领域效果优良，
并催生出在视觉领域的一系列运用~\cite{vrwkv}~\cite{RSRWKV}。同时，一些研究人员，也探索了RWKV在医学分割领域的运用~\cite{urwkv}~\cite{rwkv unet}~\cite{zigrir}。

以此为契机，我们继续完善专精于医学分割的RWKV。经过研究，我们发现，RWKV不适宜直接运用至分割任务中，因为原始的因果感受野的机制，在图像任务中表现不好，
同时，各种针对图像分割的改进，没有缓解贪吃蛇效应\ref{fig:贪吃蛇效应}，这导致边界分割模糊，没有达到像素级分割的要求\ref{fig:对比图，分割的边界锯齿明显}，
全局注意力的上下文关系也比较割裂。为了解决该问题，我们提出了Stage scan structure，一种基于Stage的RWKV扫描方法，并将一次处理分为全局block和局部block，
以此，针对特征信息的大小，进行精准的提取，保留了空间连续性。同时，我们大幅改进了Q-shift，四向位移机制，使其动态调整位移方向，有效缓解贪吃蛇效应。额外地，在原有的
Bi-WKV基础上，我们利用低分辨率的WKV实现进一步的轻量化。


\section{Methods}
\subsection{Stage Scan Structure}
Let $\mathcal{W}=\{W_l\}_{l=1}^L$ denote the weights of an $L$-layer network.
Our goal is to find a sparse mask $M_l$ for each layer such that the remaining weights $W_l\odot M_l$ retain accuracy.

\subsection{Low-Resolution WKV}
We use the gradient magnitude as the saliency score:
\begin{equation}
s_{ij}^{(l)}=\left|\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}}\right|.
\end{equation}


\section{Experiments}
We evaluate DST on CIFAR-10/100 and ImageNet with ResNet-50.

\begin{table}[h]
\centering
\caption{Top-1 accuracy (\%) on CIFAR-10 under different sparsities.}
\begin{tabular}{lcc}
\toprule
Method & 90\% sparsity & 95\% sparsity \\
\midrule
Baseline & 93.5 & 91.2 \\
Magnitude~\cite{han2015deep} & 92.8 & 89.7 \\
DST (ours) & \textbf{94.1} & \textbf{92.3} \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}
We presented a simple yet effective DST framework that dynamically adjusts sparse connectivity during training.
Future work includes extending DST to transformer architectures.

\section*{Acknowledgment}
This work was supported by the National Natural Science Foundation of China under Grant 62XXXXXX.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{99}
\bibliography{refs}
\end{thebibliography}



\end{document}
% ================= end of main.tex =================