\documentclass[conference]{IEEEtran}   % 单栏会议模板；去 conference 选项即双栏
\usepackage[UTF8, scheme = plain]{ctex} % 中文支持
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% ---------------- 常用 AI 宏包 ----------------
\usepackage{amsmath, amssymb, amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}   % 伪代码
\usepackage{graphicx}
\usepackage{booktabs}        % 三线表
\usepackage{multirow}
\usepackage{array}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{cite}            % IEEE 引用风格
\usepackage{xcolor}
\usepackage{microtype}

% ---------------- 定理环境 ----------------
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}

% ---------------- 自定义命令 ----------------
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\etal}{\textit{et al.}}

% ---------------- 正文开始 ----------------
\begin{document}

\title{基于动态扫描机制的Vision RWKV让医学影像分割轻量而高效}

\author{%
  \IEEEauthorblockN{sora\IEEEauthorrefmark{1},
                    Zhendong Li\IEEEauthorrefmark{1},
                    }
  \IEEEauthorblockA{\IEEEauthorrefmark{1}Ningxia University\\
                    Email: kyochilian@gmail.com}
  \IEEEauthorblockA{\IEEEauthorrefmark{1}Ningxia University\\
                    Email: lizhendong@nxu.edu.cn}
}

\maketitle

\begin{abstract}
基于Transformer的分割框架是当前医学影像分割的主流方法，能够构建全局关系。
然而，在需要高精度与高分辨率的医学图像分割中，Transformer计算复杂度高，应用有限。
最近的RWKV模型降低了空间聚合复杂度并具备全局处理能力，但其在视觉领域存在长程建模羸弱的问题，同时循坏嵌套机制在分割领域中的解码性能不足。
为了解决这些问题，我们提出了一种基于Vision RWKV的分割框架---，该框架在具有较低复杂度的同时具备了全局建模能力。
为了增强RWKV的长程空间连续性，我们设计了一种动态扫描机制，该机制将图像分为全局block与局部block，根据不同层次的编码器动态地调整顺序和方向，
将全局与局部特征融合，大大提升了模型的精度。
同时，我们在解码器中构建了上采样模块---，该模块改良了RWKV的循环嵌套机制，提升了重建多尺度特征的能力。
我们还采用多种方法进一步降低---的计算复杂度，实现模型的轻量化。（LRFormer 低分辨率自注意力，池化QKV等等）
实验表明，我们的方法在多个数据集取得优异性能，同时计算复杂度与计算速度显著增强，使其在高精度分割的同时具备轻量化特性。
代码开源至：\url{https://github.com/shepherdxu/SCI-winning}。
\end{abstract}

\begin{IEEEkeywords}
computer vision, semantic segmentation, RWKV, Transformer, lightweight
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{S}{emantic} Segmentation 是计算机视觉中的一项重要任务，医学影像分割是关键应用之一。它将复杂的医学图像分割成不同的区域，
使器官、病灶及注意区域清晰可见，对于医学辅助诊断与治疗具有重要意义。
相较于人工，使用计算机视觉技术进行辅助诊断不仅提高了诊断效率，还提升了诊断精度，因此许多方法被提出用于医学影像分割任务。
多年来，传统的ML（机器学习）方法在研究中被广泛应用，如基于图割（Graph Cut）的方法~\cite{Graph Cut}、随机森林（Random Forest）~\cite{Random Forest}、
支持向量机(Support Vector Machine, SVM)~\cite{SVM}以及条件随机场(Conditional Random Field, CRF)~\cite{Conditional Random Field}等。
这些方法依赖人工设计的特征提取器,在处理复杂医学图像时存在局限性，
在面对高分辨率的医学图像时,计算效率低下,制约了其在临床实践中的应用。

近年来，随着CNN(Convolution Natural Network)~\cite{CNN}、FCN(Full Convolution Network)的提出~\cite{FCN}，深度学习方法逐渐代替了传统机器学习方法，
语义分割方法的网络深度、特征提取均有了较大提高。
随着Unet~\cite{Unet}的提出，基于深度学习的分割方法占据了主导地位，通过创新性的U型架构，使其成为了语义分割的主流框架，涌现了Unet++~\cite{unet++}、
Attention-Unet~\cite{attetion-unet}等方法。
2017年，Google提出的Transformer~\cite{Transformer}在自然语言处理（NLP）领域have maked深远影响，which为视觉领域带来了ViT(Vision Transformer)~\cite{ViT} in 2020，
使得计算机视觉领域拥有了全局注意力(Global Attention)与自注意力机制(Self-Attention)~\cite{Transformer}，makes better for全局语义信息提取，改善了CNN的不足~\cite{lack for CNN}。
**等人提出了TransUnet~\cite{TransUnet}，将Unet与ViT结合起来，使得医学图像分割方法的精度大大提高。

目前，主流的方法主要遵循TransUnet的思路，在上述深度学习方法中作修改~\cite{分割方法综述}，将CNN作为局部注意力提取~\cite{EViT-Unet}，为了进行更高精度的分辨，
将Transformer作为把握全局特征的模块~\cite{SwinTransformer}，采用类似Unet的上下采样与skip connection的结构，达到期望的分割效果。
然而，医学图像具有低对比度、高分辨率、目标边界模糊等诸多问题~\cite{feature of medical img}，对于自注意力机制来说缺乏友好，往往需要花费大量的参数来提取低对比度医学特征，
牺牲了时间与空间性能~\cite{lack of Transformer on medical image}，
同时自注意力机制具有平方复杂度，高分辨率的医学图像会使计算复杂度大大增加，高参数网络也不适宜部署在医院等边缘设备，这些限制了Transformer在医学图像上的直接应用~\cite{transunet}
~\cite{h2former}。

一些研究人员研究改善这种情况。****等人提出了Linear Transformer~\cite{Linear Transformer}，**等人在20xx年提出了Mamba~\cite{Mamba}模型，不同于Transformer的框架，
他们均通过将自注意力机制的复杂度改为线性，从而缓解计算复杂度问题。然而，事实证明，线性复杂度的Transformer变体总会牺牲分割精度与准确率，而Mamba模型的长序列建模（SSM）
也不适用于视觉问题~\cite{do we really need mamba in vision problem}。鉴于医学影像中 3D 体数据、MRI与CT等高分辨率数据普遍存在，且存在高精度的像素级分割要求，
实践中还需要考虑医院边缘设备的部署问题，如何平衡精度与效率、参数与性能，成为急需解决的问题。

Recurrent Weighted Key-Value(RWKV)~\cite{rwkv}的出现引起了我们的关注。其在NLP领域的线性复杂度注意力机制，不同于Transformer的结构，使其有价值迁移到视觉任务上。
Vision RWKV(VRWKV)~\cite{visionrwkv}尝试了该问题，并针对图像输入进行了结构改进，展现出优异的计算效率与模型精度，**等人提出了BSBP-RWKV~\cite{BSBP-RWKV}，
首次将RWKV应用于医学图像任务，一些研究如RWKV-Unet~\cite{RWKV-UNet}将VRWKV与Unet迁移到医学图像分割任务上。在高分辨率的医学图像任务下，
RWKV优于其他Transformer变体与Mamba模型，处理医学图像输入时的推理速度更高，且效果更好~\cite{rwkv-sam}。

作为一款线性注意力模型，尽管RWKV拥有良好的计算效率与精度，但迁移到图像任务上，还是有不可避免的问题。RWKV本质是沿序列建模的状态空间式结构,
针对离散数据(如自然语言处理)的处理过程是固定的，自注意力的处理方式也是一维的。面对连续且二维的图像，基于RWKV的图像处理方法是将图像分块，
并延展为一维，导致了空间连续性上的破坏~\cite{zigrir}，我们称其为贪吃蛇效应~\ref{fig:贪吃蛇效应}。
RWKV的扫描方式是固定的，在图像任务中，注意力的权重往往动态且多变，固定的扫描方式往往会将注意力分片~\cite{visionrwkv}。(如图~\ref{fig:图RWKV的空间连续性破坏})。
若提取到更深层次，图像的特征更容易模糊，RWKV容易将模糊的特征边缘分离，进行分片扫描，导致容易导致器官边缘、细小病灶（如微小息肉、早期病变区）分割不精细，
而这是医学图像分割的要求之一(如图~\ref{fig:RWKV分片将深层次的边缘分割开来})。
同时，医学方向涉及三维数据，在三维医学图像中，边界分割、注意力分片的现象将会更为突出，这为RWKV向医学领域的适配带来了挑战~\cite{med-urwkv}。


为此，很多工作进行了改进。U-RWKV提出了方向自适应 RWKV 模块~\cite{urwkv}，改进了RWKV的扫描方式，使其不仅仅局限于一维层面的注意力叠加。Zig-RiR~\cite{zigrir}提出了
ZigZag的扫描方案，试图解决RWKV扫描的空间连续性问题。然而，其带来了诸多问题：扫描缺乏对图像的动态调整，导致其训练过程不稳定，且不同器官之间的精度存在较大差异
~\ref{fig:ZigRiR与URWKV的训练loss曲线不稳定};同时，贪吃蛇效应仍然存在；两者工作的上采样过于简单，导致恢复特征分辨率时的精度不足，无法满足医学图像像素级分割的要求。

我们的研究致力于解决该问题。受到Vision RWKV~\cite{vrwkv}以及URWKV~\cite{urwkv}的启发，我们提出了一种基于RWKV的U型架构invictus，该架构具备完备的编码器-解码器，
在保持了RWKV的长程依赖和线性复杂度的同时，增强了局部细节的表达与分割的精度。具体来说，我们采用了Bi-WKV，一种线性注意力，作为我们的注意力核心~\cite{vrwkv}，
并提出了Dynamic scan，一种动态扫描机制，根据不同层次的编码器动态地调整顺序和方向。
为了解决RWKV的空间动态连续性问题，invictus将一个维度的模块设置为全局block与局部block。随着编码器的层次增加，输入特征的注意力将会逐渐集中，
图像的分块数也会随着层数的增加而减少~\ref{fig:不同层次的图像分块}。在一个模块的前端，
我们使用全局block进行初步的特征提取。随后，采用残差机制~\cite{resnet}将提取后的全局权重与原特征加和，局部block根据加和后的权重，动态地调整扫描顺序与扫描方向，进行局部
的特征提取。同时，为了缓解贪吃蛇效应~\ref{fig:贪吃蛇效应}，我们改进了四项token位移~\cite{vrwkv}，根据已确定的扫描机制，动态调整扫描位移的权重与方向。

待写：
上采样问题
轻量化问题 所谓的动态？




加入RWKV扫描后的热力图，表示注意力在图像上的衰减



\section{Related Works}
\subsection{Magnitude-based Pruning}
Han \etal~\cite{han2015deep} proposed to remove weights with small absolute values.

\subsection{Dynamic Sparse Training}
Most recently, DST~\cite{evci2020rigging} allows the sparse topology to evolve during training.

\section{Methods}
Let $\mathcal{W}=\{W_l\}_{l=1}^L$ denote the weights of an $L$-layer network.
Our goal is to find a sparse mask $M_l$ for each layer such that the remaining weights $W_l\odot M_l$ retain accuracy.

\subsection{Dynamic Growth Criterion}
We use the gradient magnitude as the saliency score:
\begin{equation}
s_{ij}^{(l)}=\left|\frac{\partial \mathcal{L}}{\partial w_{ij}^{(l)}}\right|.
\end{equation}


\section{Experiments}
We evaluate DST on CIFAR-10/100 and ImageNet with ResNet-50.

\begin{table}[h]
\centering
\caption{Top-1 accuracy (\%) on CIFAR-10 under different sparsities.}
\begin{tabular}{lcc}
\toprule
Method & 90\% sparsity & 95\% sparsity \\
\midrule
Baseline & 93.5 & 91.2 \\
Magnitude~\cite{han2015deep} & 92.8 & 89.7 \\
DST (ours) & \textbf{94.1} & \textbf{92.3} \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}
We presented a simple yet effective DST framework that dynamically adjusts sparse connectivity during training.
Future work includes extending DST to transformer architectures.

\section*{Acknowledgment}
This work was supported by the National Natural Science Foundation of China under Grant 62XXXXXX.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{99}



\bibitem{han2015deep}
S.~Han, J.~Pool, J.~Tran, and W.~Dally,
``Learning both weights and connections for efficient neural network,''
\textit{Proc. NIPS}, 2015.

\bibitem{evci2020rigging}
U.~Evci, T.~Gale, J.~Menick, P.~S.~Castro, and E.~Elsen,
``Rigging the lottery: Making all tickets winners,''
\textit{Proc. ICML}, 2020.


\bibitem{FCN}

\bibitem{Graph Cut}

\bibitem{Random Forest}

\bibitem{SVM}

\bibitem{Conditional Random Field}

\bibitem{CNN}

\bibitem{FCN}

\bibitem{Unet}

\bibitem{Transformer}

\bibitem{ViT}


\end{thebibliography}



\end{document}
% ================= end of main.tex =================